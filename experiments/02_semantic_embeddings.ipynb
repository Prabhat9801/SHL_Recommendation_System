{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 2: Adding Semantic Embeddings\n",
        "\n",
        "\n",
        "\n",
        "Previous experiment got 19% with just TF-IDF. That's better than random but not good enough.\n",
        "\n",
        "**Idea:** Maybe semantic embeddings can capture meaning better?\n",
        "- \"collaborate\" should match \"teamwork\"\n",
        "- \"Java\" should match \"programming\"\n",
        "\n",
        "Let's try Sentence-BERT!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "df = pd.read_csv('../data/shl_individual_test_solutions.csv')\n",
        "train_df = pd.read_excel('../data/Gen_AI Dataset (1).xlsx', sheet_name='Train-Set')\n",
        "\n",
        "# URL normalization\n",
        "df['normalized_url'] = df['url'].str.replace('/solutions/products/', '/products/')\n",
        "train_df['normalized_url'] = train_df['Assessment_url'].str.replace('/solutions/products/', '/products/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "Model loaded!\n"
          ]
        }
      ],
      "source": [
        "# Load embedding model\n",
        "# Using MiniLM - supposed to be good for similarity\n",
        "print(\"Loading model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Model loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoding 377 texts...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3146078637c646a39c77cb3df23fe757",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embeddings shape: (377, 384)\n"
          ]
        }
      ],
      "source": [
        "# Create richer text for embeddings\n",
        "texts = []\n",
        "for _, row in df.iterrows():\n",
        "    text = f\"{row['name']}. {row['description']}.\"\n",
        "    texts.append(text)\n",
        "\n",
        "print(f\"Encoding {len(texts)} texts...\")\n",
        "embeddings = model.encode(texts, show_progress_bar=True)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test semantic similarity\n",
        "\n",
        "Let's see if \"collaborate\" matches personality tests..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 10 semantic matches for 'developer who collaborates':\n",
            "1. Digital Readiness Development Report - IC (score: 0.426)\n",
            "2. Salesforce Development (New) (score: 0.417)\n",
            "3. Digital Readiness Development Report - Manager (score: 0.377)\n",
            "4. OPQ Team Impact Individual Development Report (score: 0.356)\n",
            "5. RemoteWorkQ (score: 0.320)\n",
            "6. Android Development (New) (score: 0.318)\n",
            "7. OPQ UCF Development Action Planner Report 1.0 (score: 0.318)\n",
            "8. RemoteWorkQ Participant Report (score: 0.318)\n",
            "9. Siebel Development (New) (score: 0.317)\n",
            "10. GIT (New) (score: 0.316)\n"
          ]
        }
      ],
      "source": [
        "query = \"developer who collaborates\"\n",
        "query_emb = model.encode([query])\n",
        "semantic_scores = cosine_similarity(query_emb, embeddings)[0]\n",
        "\n",
        "top_10_idx = np.argsort(semantic_scores)[-10:][::-1]\n",
        "\n",
        "print(f\"Top 10 semantic matches for '{query}':\")\n",
        "for i, idx in enumerate(top_10_idx, 1):\n",
        "    print(f\"{i}. {df.iloc[idx]['name']} (score: {semantic_scores[idx]:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interesting!** Getting some personality tests now (OPQ, etc.)\n",
        "\n",
        "But losing some technical tests...\n",
        "\n",
        "**Idea:** Combine TF-IDF + Semantic?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build TF-IDF too\n",
        "documents = [f\"{row['name']} {row['description']}\" for _, row in df.iterrows()]\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall: 0.00 | Query: Based on the JD below recommend me assessment for the Consul...\n",
            "Recall: 0.60 | Query: Content Writer required, expert in English and SEO....\n",
            "Recall: 0.00 | Query: Find me 1 hour long assesment for the below job at SHL\n",
            "Job D...\n",
            "Recall: 0.20 | Query: I am hiring for Java developers who can also collaborate eff...\n",
            "Recall: 0.00 | Query: I am looking for a COO for my company in China and I want to...\n",
            "Recall: 0.30 | Query: I want to hire a Senior Data Analyst with 5 years of experie...\n",
            "Recall: 0.11 | Query: I want to hire new graduates for a sales role in my company,...\n",
            "Recall: 0.00 | Query: ICICI Bank Assistant Admin, Experience required 0-2 years, t...\n",
            "Recall: 0.20 | Query: KEY RESPONSIBITILES:\n",
            "\n",
            "Manage the sound-scape of the station ...\n",
            "Recall: 0.00 | Query: We're looking for a Marketing Manager who can drive Recroâ€™s ...\n",
            "\n",
            "Mean Recall@10: 0.141 (14.1%)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate with hybrid: 50% TF-IDF + 50% Semantic\n",
        "recalls = []\n",
        "\n",
        "for query, group in train_df.groupby('Query'):\n",
        "    ground_truth = set(group['normalized_url'])\n",
        "    \n",
        "    # TF-IDF scores\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    tfidf_scores = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
        "    \n",
        "    # Semantic scores\n",
        "    query_emb = model.encode([query])\n",
        "    semantic_scores = cosine_similarity(query_emb, embeddings)[0]\n",
        "    \n",
        "    # Combine (trying 50-50 split)\n",
        "    combined_scores = 0.5 * tfidf_scores + 0.5 * semantic_scores\n",
        "    \n",
        "    top_10_idx = np.argsort(combined_scores)[-10:][::-1]\n",
        "    predicted = set(df.iloc[top_10_idx]['normalized_url'])\n",
        "    \n",
        "    found = len(ground_truth & predicted)\n",
        "    recall = found / len(ground_truth) if len(ground_truth) > 0 else 0\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    print(f\"Recall: {recall:.2f} | Query: {query[:60]}...\")\n",
        "\n",
        "mean_recall = np.mean(recalls)\n",
        "print(f\"\\nMean Recall@10: {mean_recall:.3f} ({mean_recall*100:.1f}%)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (fastapi-mcq-venv)",
      "language": "python",
      "name": "fastapi-mcq-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
