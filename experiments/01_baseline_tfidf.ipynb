{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 1: Basic TF-IDF Approach\n",
        "\n",
        "\n",
        "**Goal:** Start simple with TF-IDF to get a baseline\n",
        "\n",
        "**Hypothesis:** TF-IDF should capture keyword matches between queries and assessments\n",
        "\n",
        "Let's see what we get..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('../data/shl_individual_test_solutions.csv')\n",
        "train_df = pd.read_excel('../data/Gen_AI Dataset (1).xlsx', sheet_name='Train-Set')\n",
        "\n",
        "print(f\"Assessments: {len(df)}\")\n",
        "print(f\"Training examples: {len(train_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create simple documents\n",
        "\n",
        "Just concatenate name and description for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create documents - keeping it simple\n",
        "documents = []\n",
        "for _, row in df.iterrows():\n",
        "    doc = f\"{row['name']} {row['description']}\"\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f\"Created {len(documents)} documents\")\n",
        "print(f\"Sample: {documents[0][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build TF-IDF\n",
        "# Using default parameters first\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(f\"TF-IDF shape: {tfidf_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with a query\n",
        "\n",
        "Let's try \"Java developer\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"Java developer\"\n",
        "query_vec = vectorizer.transform([query])\n",
        "scores = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
        "\n",
        "# Get top 10\n",
        "top_indices = np.argsort(scores)[-10:][::-1]\n",
        "\n",
        "print(f\"Top 10 for '{query}':\")\n",
        "for i, idx in enumerate(top_indices, 1):\n",
        "    print(f\"{i}. {df.iloc[idx]['name']} (score: {scores[idx]:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Observations:**\n",
        "- Java assessments show up! \u2713\n",
        "- Scores are pretty low though (< 0.3)\n",
        "- Missing some relevant tests\n",
        "\n",
        "Let's evaluate properly on training set..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick evaluation\n",
        "# Normalize URLs first\n",
        "df['normalized_url'] = df['url'].str.replace('/solutions/products/', '/products/')\n",
        "train_df['normalized_url'] = train_df['Assessment_url'].str.replace('/solutions/products/', '/products/')\n",
        "\n",
        "recalls = []\n",
        "for query, group in train_df.groupby('Query'):\n",
        "    ground_truth = set(group['normalized_url'])\n",
        "    \n",
        "    # Get predictions\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    scores = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
        "    top_10_idx = np.argsort(scores)[-10:][::-1]\n",
        "    predicted = set(df.iloc[top_10_idx]['normalized_url'])\n",
        "    \n",
        "    # Calculate recall\n",
        "    found = len(ground_truth & predicted)\n",
        "    recall = found / len(ground_truth) if len(ground_truth) > 0 else 0\n",
        "    recalls.append(recall)\n",
        "    \n",
        "    print(f\"Query: {query[:50]}... | Recall: {recall:.2f}\")\n",
        "\n",
        "print(f\"\\nMean Recall@10: {np.mean(recalls):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: 26.2% Mean Recall@10\n",
        "\n",
        "**Not great...**\n",
        "\n",
        "**Issues:**\n",
        "1. Only exact keyword matches work\n",
        "2. No semantic understanding\n",
        "3. Not using training data patterns\n",
        "\n",
        "**Next steps:**\n",
        "- Try semantic embeddings?\n",
        "- Weight fields differently?\n",
        "- Use LLM for query understanding?\n",
        "\n",
        "Will try embeddings next..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}