{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 3: LLM for Query Understanding\n",
        "\n",
        "\n",
        "32% is still not good enough. Maybe the problem is we don't understand the QUERY well?\n",
        "\n",
        "Example: \"Java developer who collaborates\"\n",
        "- We need: Technical tests (Java) + Soft skill tests (collaboration)\n",
        "- System doesn't know to look for BOTH\n",
        "\n",
        "**Hypothesis:** LLM can extract structured requirements from natural language\n",
        "\n",
        "Let's try Groq (free, fast!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from groq import Groq\n",
        "import json\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "client = Groq(api_key=os.getenv('GROQ_API_KEY'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test LLM extraction\n",
        "query = \"I need Java developers who can collaborate with business teams\"\n",
        "\n",
        "prompt = f'''Extract requirements from this job query:\n",
        "\"{query}\"\n",
        "\n",
        "Return JSON with:\n",
        "{{\n",
        "    \"technical_skills\": [\"skill1\", \"skill2\"],\n",
        "    \"soft_skills\": [\"skill1\", \"skill2\"],\n",
        "    \"role_type\": \"developer/analyst/etc\",\n",
        "    \"keywords\": [\"word1\", \"word2\"]\n",
        "}}'''\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "result = response.choices[0].message.content\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Wow!** It extracted:\n",
        "- Technical: Java, programming\n",
        "- Soft: collaboration, teamwork\n",
        "- Role: developer\n",
        "\n",
        "This could help boost relevant assessments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load existing system\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "df = pd.read_csv('../data/shl_individual_test_solutions.csv')\n",
        "train_df = pd.read_excel('../data/Gen_AI Dataset (1).xlsx', sheet_name='Train-Set')\n",
        "\n",
        "df['normalized_url'] = df['url'].str.replace('/solutions/products/', '/products/')\n",
        "train_df['normalized_url'] = train_df['Assessment_url'].str.replace('/solutions/products/', '/products/')\n",
        "\n",
        "# Build features\n",
        "documents = [f\"{row['name']} {row['description']}\" for _, row in df.iterrows()]\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "texts = [f\"{row['name']}. {row['description']}.\" for _, row in df.iterrows()]\n",
        "embeddings = model.encode(texts, show_progress_bar=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate with LLM extraction\n",
        "def extract_with_llm(query):\n",
        "    try:\n",
        "        prompt = f'Extract skills from: \"{query}\" Return JSON with technical_skills and soft_skills arrays'\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=0\n",
        "        )\n",
        "        text = response.choices[0].message.content\n",
        "        # Parse JSON\n",
        "        if '```' in text:\n",
        "            text = text.split('```')[1].replace('json', '').strip()\n",
        "        return json.loads(text)\n",
        "    except:\n",
        "        return {\"technical_skills\": [], \"soft_skills\": []}\n",
        "\n",
        "recalls = []\n",
        "for query, group in train_df.groupby('Query'):\n",
        "    ground_truth = set(group['normalized_url'])\n",
        "    \n",
        "    # Extract with LLM\n",
        "    llm_data = extract_with_llm(query)\n",
        "    \n",
        "    # Base scores\n",
        "    query_vec = vectorizer.transform([query])\n",
        "    tfidf_scores = cosine_similarity(query_vec, tfidf_matrix)[0]\n",
        "    query_emb = model.encode([query])\n",
        "    semantic_scores = cosine_similarity(query_emb, embeddings)[0]\n",
        "    \n",
        "    # Add LLM boosts\n",
        "    final_scores = 0.4 * tfidf_scores + 0.4 * semantic_scores\n",
        "    \n",
        "    for i, row in df.iterrows():\n",
        "        name_lower = row['name'].lower()\n",
        "        # Boost if LLM-extracted skills match\n",
        "        for skill in llm_data.get('technical_skills', []):\n",
        "            if skill.lower() in name_lower:\n",
        "                final_scores[i] += 0.1\n",
        "        for skill in llm_data.get('soft_skills', []):\n",
        "            if skill.lower() in name_lower:\n",
        "                final_scores[i] += 0.1\n",
        "    \n",
        "    top_10_idx = np.argsort(final_scores)[-10:][::-1]\n",
        "    predicted = set(df.iloc[top_10_idx]['normalized_url'])\n",
        "    \n",
        "    recall = len(ground_truth & predicted) / len(ground_truth)\n",
        "    recalls.append(recall)\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "mean_recall = np.mean(recalls)\n",
        "print(f\"\\nMean Recall@10 with LLM: {mean_recall:.3f} ({mean_recall*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results: 36.0% Mean Recall@10\n",
        "\n",
        "**Small improvement** (+3.2%)\n",
        "\n",
        "LLM helps but not as much as I hoped...\n",
        "\n",
        "**Problem:** We're still ignoring the training data!\n",
        "\n",
        "The training set shows what hiring managers ACTUALLY chose - that's gold!\n",
        "\n",
        "Next: Learn patterns from training data \ud83d\udca1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}